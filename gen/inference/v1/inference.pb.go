// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v4.25.1
// source: inference/v1/inference.proto

package inferencev1

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Priority levels for QoS
type Priority int32

const (
	Priority_LOW    Priority = 0
	Priority_MEDIUM Priority = 1
	Priority_HIGH   Priority = 2
)

// Enum value maps for Priority.
var (
	Priority_name = map[int32]string{
		0: "LOW",
		1: "MEDIUM",
		2: "HIGH",
	}
	Priority_value = map[string]int32{
		"LOW":    0,
		"MEDIUM": 1,
		"HIGH":   2,
	}
)

func (x Priority) Enum() *Priority {
	p := new(Priority)
	*p = x
	return p
}

func (x Priority) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Priority) Descriptor() protoreflect.EnumDescriptor {
	return file_inference_v1_inference_proto_enumTypes[0].Descriptor()
}

func (Priority) Type() protoreflect.EnumType {
	return &file_inference_v1_inference_proto_enumTypes[0]
}

func (x Priority) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Priority.Descriptor instead.
func (Priority) EnumDescriptor() ([]byte, []int) {
	return file_inference_v1_inference_proto_rawDescGZIP(), []int{0}
}

type InferRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Payload       []byte                 `protobuf:"bytes,2,opt,name=payload,proto3" json:"payload,omitempty"`                      // image bytes or tensor data
	Timestamp     int64                  `protobuf:"varint,3,opt,name=timestamp,proto3" json:"timestamp,omitempty"`                 // unix nanos
	ModelName     string                 `protobuf:"bytes,4,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"` // e.g. "resnet50"
	Priority      Priority               `protobuf:"varint,5,opt,name=priority,proto3,enum=inference.v1.Priority" json:"priority,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferRequest) Reset() {
	*x = InferRequest{}
	mi := &file_inference_v1_inference_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferRequest) ProtoMessage() {}

func (x *InferRequest) ProtoReflect() protoreflect.Message {
	mi := &file_inference_v1_inference_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferRequest.ProtoReflect.Descriptor instead.
func (*InferRequest) Descriptor() ([]byte, []int) {
	return file_inference_v1_inference_proto_rawDescGZIP(), []int{0}
}

func (x *InferRequest) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *InferRequest) GetPayload() []byte {
	if x != nil {
		return x.Payload
	}
	return nil
}

func (x *InferRequest) GetTimestamp() int64 {
	if x != nil {
		return x.Timestamp
	}
	return 0
}

func (x *InferRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *InferRequest) GetPriority() Priority {
	if x != nil {
		return x.Priority
	}
	return Priority_LOW
}

type InferResponse struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	RequestId     string                 `protobuf:"bytes,1,opt,name=request_id,json=requestId,proto3" json:"request_id,omitempty"`
	Result        []byte                 `protobuf:"bytes,2,opt,name=result,proto3" json:"result,omitempty"`
	LatencyNs     int64                  `protobuf:"varint,3,opt,name=latency_ns,json=latencyNs,proto3" json:"latency_ns,omitempty"`
	WorkerId      string                 `protobuf:"bytes,4,opt,name=worker_id,json=workerId,proto3" json:"worker_id,omitempty"`
	BatchSize     int32                  `protobuf:"varint,5,opt,name=batch_size,json=batchSize,proto3" json:"batch_size,omitempty"`         // how many were batched together
	QueueWaitMs   int32                  `protobuf:"varint,6,opt,name=queue_wait_ms,json=queueWaitMs,proto3" json:"queue_wait_ms,omitempty"` // time spent in queue
	PriorityUsed  string                 `protobuf:"bytes,7,opt,name=priority_used,json=priorityUsed,proto3" json:"priority_used,omitempty"` // echoed back
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferResponse) Reset() {
	*x = InferResponse{}
	mi := &file_inference_v1_inference_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferResponse) ProtoMessage() {}

func (x *InferResponse) ProtoReflect() protoreflect.Message {
	mi := &file_inference_v1_inference_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferResponse.ProtoReflect.Descriptor instead.
func (*InferResponse) Descriptor() ([]byte, []int) {
	return file_inference_v1_inference_proto_rawDescGZIP(), []int{1}
}

func (x *InferResponse) GetRequestId() string {
	if x != nil {
		return x.RequestId
	}
	return ""
}

func (x *InferResponse) GetResult() []byte {
	if x != nil {
		return x.Result
	}
	return nil
}

func (x *InferResponse) GetLatencyNs() int64 {
	if x != nil {
		return x.LatencyNs
	}
	return 0
}

func (x *InferResponse) GetWorkerId() string {
	if x != nil {
		return x.WorkerId
	}
	return ""
}

func (x *InferResponse) GetBatchSize() int32 {
	if x != nil {
		return x.BatchSize
	}
	return 0
}

func (x *InferResponse) GetQueueWaitMs() int32 {
	if x != nil {
		return x.QueueWaitMs
	}
	return 0
}

func (x *InferResponse) GetPriorityUsed() string {
	if x != nil {
		return x.PriorityUsed
	}
	return ""
}

type MetricsRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *MetricsRequest) Reset() {
	*x = MetricsRequest{}
	mi := &file_inference_v1_inference_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *MetricsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*MetricsRequest) ProtoMessage() {}

func (x *MetricsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_inference_v1_inference_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use MetricsRequest.ProtoReflect.Descriptor instead.
func (*MetricsRequest) Descriptor() ([]byte, []int) {
	return file_inference_v1_inference_proto_rawDescGZIP(), []int{2}
}

type WorkerMetrics struct {
	state          protoimpl.MessageState `protogen:"open.v1"`
	WorkerId       string                 `protobuf:"bytes,1,opt,name=worker_id,json=workerId,proto3" json:"worker_id,omitempty"`
	VramFreeGb     float64                `protobuf:"fixed64,2,opt,name=vram_free_gb,json=vramFreeGb,proto3" json:"vram_free_gb,omitempty"`
	VramTotalGb    float64                `protobuf:"fixed64,3,opt,name=vram_total_gb,json=vramTotalGb,proto3" json:"vram_total_gb,omitempty"`
	QueueDepth     int32                  `protobuf:"varint,4,opt,name=queue_depth,json=queueDepth,proto3" json:"queue_depth,omitempty"`
	AvgLatencyMs   float64                `protobuf:"fixed64,5,opt,name=avg_latency_ms,json=avgLatencyMs,proto3" json:"avg_latency_ms,omitempty"`
	GpuUtilization float64                `protobuf:"fixed64,6,opt,name=gpu_utilization,json=gpuUtilization,proto3" json:"gpu_utilization,omitempty"` // 0-100
	TemperatureC   float64                `protobuf:"fixed64,7,opt,name=temperature_c,json=temperatureC,proto3" json:"temperature_c,omitempty"`
	CurrentBatch   int32                  `protobuf:"varint,8,opt,name=current_batch,json=currentBatch,proto3" json:"current_batch,omitempty"`
	Healthy        bool                   `protobuf:"varint,9,opt,name=healthy,proto3" json:"healthy,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *WorkerMetrics) Reset() {
	*x = WorkerMetrics{}
	mi := &file_inference_v1_inference_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *WorkerMetrics) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*WorkerMetrics) ProtoMessage() {}

func (x *WorkerMetrics) ProtoReflect() protoreflect.Message {
	mi := &file_inference_v1_inference_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use WorkerMetrics.ProtoReflect.Descriptor instead.
func (*WorkerMetrics) Descriptor() ([]byte, []int) {
	return file_inference_v1_inference_proto_rawDescGZIP(), []int{3}
}

func (x *WorkerMetrics) GetWorkerId() string {
	if x != nil {
		return x.WorkerId
	}
	return ""
}

func (x *WorkerMetrics) GetVramFreeGb() float64 {
	if x != nil {
		return x.VramFreeGb
	}
	return 0
}

func (x *WorkerMetrics) GetVramTotalGb() float64 {
	if x != nil {
		return x.VramTotalGb
	}
	return 0
}

func (x *WorkerMetrics) GetQueueDepth() int32 {
	if x != nil {
		return x.QueueDepth
	}
	return 0
}

func (x *WorkerMetrics) GetAvgLatencyMs() float64 {
	if x != nil {
		return x.AvgLatencyMs
	}
	return 0
}

func (x *WorkerMetrics) GetGpuUtilization() float64 {
	if x != nil {
		return x.GpuUtilization
	}
	return 0
}

func (x *WorkerMetrics) GetTemperatureC() float64 {
	if x != nil {
		return x.TemperatureC
	}
	return 0
}

func (x *WorkerMetrics) GetCurrentBatch() int32 {
	if x != nil {
		return x.CurrentBatch
	}
	return 0
}

func (x *WorkerMetrics) GetHealthy() bool {
	if x != nil {
		return x.Healthy
	}
	return false
}

var File_inference_v1_inference_proto protoreflect.FileDescriptor

const file_inference_v1_inference_proto_rawDesc = "" +
	"\n" +
	"\x1cinference/v1/inference.proto\x12\finference.v1\"\xb8\x01\n" +
	"\fInferRequest\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x18\n" +
	"\apayload\x18\x02 \x01(\fR\apayload\x12\x1c\n" +
	"\ttimestamp\x18\x03 \x01(\x03R\ttimestamp\x12\x1d\n" +
	"\n" +
	"model_name\x18\x04 \x01(\tR\tmodelName\x122\n" +
	"\bpriority\x18\x05 \x01(\x0e2\x16.inference.v1.PriorityR\bpriority\"\xea\x01\n" +
	"\rInferResponse\x12\x1d\n" +
	"\n" +
	"request_id\x18\x01 \x01(\tR\trequestId\x12\x16\n" +
	"\x06result\x18\x02 \x01(\fR\x06result\x12\x1d\n" +
	"\n" +
	"latency_ns\x18\x03 \x01(\x03R\tlatencyNs\x12\x1b\n" +
	"\tworker_id\x18\x04 \x01(\tR\bworkerId\x12\x1d\n" +
	"\n" +
	"batch_size\x18\x05 \x01(\x05R\tbatchSize\x12\"\n" +
	"\rqueue_wait_ms\x18\x06 \x01(\x05R\vqueueWaitMs\x12#\n" +
	"\rpriority_used\x18\a \x01(\tR\fpriorityUsed\"\x10\n" +
	"\x0eMetricsRequest\"\xc6\x02\n" +
	"\rWorkerMetrics\x12\x1b\n" +
	"\tworker_id\x18\x01 \x01(\tR\bworkerId\x12 \n" +
	"\fvram_free_gb\x18\x02 \x01(\x01R\n" +
	"vramFreeGb\x12\"\n" +
	"\rvram_total_gb\x18\x03 \x01(\x01R\vvramTotalGb\x12\x1f\n" +
	"\vqueue_depth\x18\x04 \x01(\x05R\n" +
	"queueDepth\x12$\n" +
	"\x0eavg_latency_ms\x18\x05 \x01(\x01R\favgLatencyMs\x12'\n" +
	"\x0fgpu_utilization\x18\x06 \x01(\x01R\x0egpuUtilization\x12#\n" +
	"\rtemperature_c\x18\a \x01(\x01R\ftemperatureC\x12#\n" +
	"\rcurrent_batch\x18\b \x01(\x05R\fcurrentBatch\x12\x18\n" +
	"\ahealthy\x18\t \x01(\bR\ahealthy*)\n" +
	"\bPriority\x12\a\n" +
	"\x03LOW\x10\x00\x12\n" +
	"\n" +
	"\x06MEDIUM\x10\x01\x12\b\n" +
	"\x04HIGH\x10\x022T\n" +
	"\x10InferenceService\x12@\n" +
	"\x05Infer\x12\x1a.inference.v1.InferRequest\x1a\x1b.inference.v1.InferResponse2_\n" +
	"\x14WorkerMetricsService\x12G\n" +
	"\n" +
	"GetMetrics\x12\x1c.inference.v1.MetricsRequest\x1a\x1b.inference.v1.WorkerMetricsB@Z>github.com/kunal/gpu-batch-router/gen/inference/v1;inferencev1b\x06proto3"

var (
	file_inference_v1_inference_proto_rawDescOnce sync.Once
	file_inference_v1_inference_proto_rawDescData []byte
)

func file_inference_v1_inference_proto_rawDescGZIP() []byte {
	file_inference_v1_inference_proto_rawDescOnce.Do(func() {
		file_inference_v1_inference_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_inference_v1_inference_proto_rawDesc), len(file_inference_v1_inference_proto_rawDesc)))
	})
	return file_inference_v1_inference_proto_rawDescData
}

var file_inference_v1_inference_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_inference_v1_inference_proto_msgTypes = make([]protoimpl.MessageInfo, 4)
var file_inference_v1_inference_proto_goTypes = []any{
	(Priority)(0),          // 0: inference.v1.Priority
	(*InferRequest)(nil),   // 1: inference.v1.InferRequest
	(*InferResponse)(nil),  // 2: inference.v1.InferResponse
	(*MetricsRequest)(nil), // 3: inference.v1.MetricsRequest
	(*WorkerMetrics)(nil),  // 4: inference.v1.WorkerMetrics
}
var file_inference_v1_inference_proto_depIdxs = []int32{
	0, // 0: inference.v1.InferRequest.priority:type_name -> inference.v1.Priority
	1, // 1: inference.v1.InferenceService.Infer:input_type -> inference.v1.InferRequest
	3, // 2: inference.v1.WorkerMetricsService.GetMetrics:input_type -> inference.v1.MetricsRequest
	2, // 3: inference.v1.InferenceService.Infer:output_type -> inference.v1.InferResponse
	4, // 4: inference.v1.WorkerMetricsService.GetMetrics:output_type -> inference.v1.WorkerMetrics
	3, // [3:5] is the sub-list for method output_type
	1, // [1:3] is the sub-list for method input_type
	1, // [1:1] is the sub-list for extension type_name
	1, // [1:1] is the sub-list for extension extendee
	0, // [0:1] is the sub-list for field type_name
}

func init() { file_inference_v1_inference_proto_init() }
func file_inference_v1_inference_proto_init() {
	if File_inference_v1_inference_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_inference_v1_inference_proto_rawDesc), len(file_inference_v1_inference_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   4,
			NumExtensions: 0,
			NumServices:   2,
		},
		GoTypes:           file_inference_v1_inference_proto_goTypes,
		DependencyIndexes: file_inference_v1_inference_proto_depIdxs,
		EnumInfos:         file_inference_v1_inference_proto_enumTypes,
		MessageInfos:      file_inference_v1_inference_proto_msgTypes,
	}.Build()
	File_inference_v1_inference_proto = out.File
	file_inference_v1_inference_proto_goTypes = nil
	file_inference_v1_inference_proto_depIdxs = nil
}
