# GPU-Aware Intelligent Batch Router ğŸš€

A production-grade GPU inference routing system in Go that intelligently routes requests to the best GPU worker using real-time metrics, adaptive micro-batching, and priority queues.

**Demo target:** Google Colab (free T4 GPU) â†’ K3s â†’ NVIDIA GPU Time-Slicing (1 GPU â†’ 3 virtual GPUs)

![Control Center](pkg/router/dashboard/screenshot.png)

---

## Architecture

```
Client â”€â”€gRPCâ”€â”€â–¶ Router â”€â”€score & routeâ”€â”€â–¶ Worker 1 (vGPU-0)
                   â”‚                      Worker 2 (vGPU-1)  
                   â”‚                      Worker 3 (vGPU-2)
                   â”‚
                   â”œâ”€â”€ Scores workers by: VRAM, queue depth, latency, GPU util, temperature
                   â”œâ”€â”€ Anti-thundering-herd: weighted random among top-3
                   â”œâ”€â”€ Retry + failover: 2 retries, marks unhealthy after 3 failures
                   â””â”€â”€ Dashboard: real-time WebSocket updates at :8080
```

Each **Worker** implements:
- **Priority Queue** â€” HIGH requests skip ahead of LOW (QoS)
- **Adaptive Micro-Batching** â€” collects 1-32 requests per batch, adapts wait time based on queue pressure
- **GPU Executor** â€” real ONNX Runtime inference (ResNet-50) or simulation fallback
- **NVML Metrics** â€” real GPU temp/VRAM/utilization via CGo bindings

---

## Quick Start (Local â€” Simulated GPU)

```bash
# Start 3 workers
WORKER_ID=worker-1 WORKER_PORT=50052 METRICS_PORT=9091 go run ./cmd/worker/ &
WORKER_ID=worker-2 WORKER_PORT=50053 METRICS_PORT=9092 go run ./cmd/worker/ &
WORKER_ID=worker-3 WORKER_PORT=50054 METRICS_PORT=9093 go run ./cmd/worker/ &

# Start router
WORKER_ENDPOINTS=localhost:50052,localhost:50053,localhost:50054 go run ./cmd/router/

# Open dashboard
open http://localhost:8080

# Run load test (in another terminal)
go run scripts/loadtest.go --addr=localhost:50051 --concurrency=50 --duration=30s
```

## Deploy on Google Colab (Real T4 GPU)

1. Open a Colab notebook with **T4 GPU** runtime
2. Clone this repo:
   ```python
   !git clone https://github.com/kunal/gpu-batch-router.git
   %cd gpu-batch-router
   ```
3. Run the one-click setup:
   ```python
   !bash scripts/setup_colab.sh
   ```

This installs K3s, enables GPU time-slicing (1 T4 â†’ 3 vGPUs), builds with real ONNX + NVML, deploys to K3s, and tunnels the dashboard via ngrok.

## Docker Compose (Local)

```bash
docker compose -f deploy/docker-compose.yaml up --build
# Dashboard at http://localhost:8080
```

---

## Project Structure

```
â”œâ”€â”€ proto/inference/v1/inference.proto   # gRPC service definitions
â”œâ”€â”€ gen/inference/v1/                    # Generated Go code
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ router/main.go                  # Router entrypoint
â”‚   â””â”€â”€ worker/main.go                  # Worker entrypoint
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ router.go                   # Core routing + retry + anti-thundering-herd
â”‚   â”‚   â”œâ”€â”€ scorer.go                   # GPU scoring algorithm
â”‚   â”‚   â”œâ”€â”€ registry.go                 # Worker health tracking
â”‚   â”‚   â”œâ”€â”€ poller.go                   # Metrics polling
â”‚   â”‚   â”œâ”€â”€ broadcast.go                # WebSocket for dashboard
â”‚   â”‚   â””â”€â”€ dashboard/index.html        # Real-time control center
â”‚   â”œâ”€â”€ worker/
â”‚   â”‚   â”œâ”€â”€ server.go                   # gRPC worker server
â”‚   â”‚   â”œâ”€â”€ queue.go                    # Heap-based priority queue
â”‚   â”‚   â”œâ”€â”€ batcher.go                  # Adaptive micro-batching engine
â”‚   â”‚   â”œâ”€â”€ metrics.go                  # GPU metrics (simulated + real NVML)
â”‚   â”‚   â”œâ”€â”€ executor/                   # GPU executor (simulation + ONNX)
â”‚   â”‚   â””â”€â”€ nvml/                       # NVIDIA GPU bindings (CGo, dlopen)
â”‚   â””â”€â”€ config/config.go                # Environment-based config
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ docker-compose.yaml             # Local dev (3 workers + router)
â”‚   â”œâ”€â”€ docker/Dockerfile.*             # Multi-stage Docker builds
â”‚   â””â”€â”€ k8s/                            # K3s manifests + GPU time-slicing
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_colab.sh                  # One-click Colab deployment
â”‚   â”œâ”€â”€ loadtest.go                     # gRPC load test client
â”‚   â””â”€â”€ gen-proto.sh                    # Proto code generation
â””â”€â”€ Makefile
```

## Configuration (Environment Variables)

| Variable | Default | Description |
|----------|---------|-------------|
| `WORKER_ID` | `worker-0` | Unique worker identifier |
| `ROUTER_PORT` | `50051` | Router gRPC port |
| `WORKER_PORT` | `50052` | Worker gRPC port |
| `DASHBOARD_PORT` | `8080` | Dashboard HTTP port |
| `METRICS_PORT` | `9090` | Prometheus metrics port |
| `MAX_BATCH_SIZE` | `32` | Maximum batch size |
| `MAX_WAIT_MS` | `50` | Max time to wait for batch to fill (ms) |
| `POLL_INTERVAL_MS` | `500` | How often router polls worker metrics |
| `WORKER_ENDPOINTS` | â€” | Comma-separated worker addresses |
| `EXECUTOR_TYPE` | `simulation` | `simulation` or `onnx` |
| `USE_NVML` | `auto` | `auto`, `true`, or `false` |
| `ONNX_MODEL_PATH` | `/models/resnet50.onnx` | Path to ONNX model file |

## Build Tags

| Tag | Effect |
|-----|--------|
| (none) | Simulation mode â€” works everywhere |
| `-tags onnx` | Real ONNX Runtime inference (requires libonnxruntime) |
| `-tags nvml` | Real NVIDIA GPU metrics (requires libnvidia-ml.so) |
| `-tags "onnx,nvml"` | Full GPU mode (Colab) |

---

## What's Real vs Simulated

| Component | Local (default) | Colab (`-tags "onnx,nvml"`) |
|-----------|----------------|----------------------------|
| gRPC routing | âœ… Real | âœ… Real |
| Priority queue | âœ… Real | âœ… Real |
| Adaptive batching | âœ… Real | âœ… Real |
| Scoring algorithm | âœ… Real | âœ… Real |
| Retry/failover | âœ… Real | âœ… Real |
| Dashboard (WebSocket) | âœ… Real | âœ… Real |
| GPU metrics | ğŸ”¶ Simulated (reactive) | âœ… Real NVML |
| AI inference | ğŸ”¶ Simulated (sleep + matrix) | âœ… Real ONNX (ResNet-50) |
| GPU hardware | âŒ CPU only | âœ… Tesla T4 (15GB) |

---

## License

MIT
