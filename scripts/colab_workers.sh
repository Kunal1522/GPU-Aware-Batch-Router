#!/bin/bash
# =============================================================================
# GPU Batch Router â€” Colab WORKERS ONLY
# 
# This script runs ONLY the GPU workers on Colab (real T4 GPU).
# Your router + dashboard runs on YOUR LOCAL machine.
#
# How it works:
#   Colab (workers) â†â”€â”€ ngrok tunnels â”€â”€â†’ Your laptop (router + dashboard)
#
# Usage in Colab:
#   !bash scripts/colab_workers.sh
# =============================================================================
set -euo pipefail

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ðŸš€ GPU Workers â€” Colab Setup (workers only)"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# --- Step 1: Verify GPU ---
echo ""
echo "ðŸ“Œ Step 1: Checking GPU..."
if ! nvidia-smi &>/dev/null; then
    echo "âŒ No GPU! Go to Runtime â†’ Change runtime type â†’ T4 GPU"
    exit 1
fi
GPU_NAME=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader | head -1)
GPU_VRAM=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader | head -1)
echo "âœ… GPU: $GPU_NAME ($GPU_VRAM)"

# --- Step 2: Install Go ---
echo ""
echo "ðŸ“Œ Step 2: Installing Go..."
if ! command -v /usr/local/go/bin/go &>/dev/null; then
    wget -q https://go.dev/dl/go1.24.0.linux-amd64.tar.gz
    sudo rm -rf /usr/local/go
    sudo tar -C /usr/local -xzf go1.24.0.linux-amd64.tar.gz
    rm go1.24.0.linux-amd64.tar.gz
fi
export PATH=/usr/local/go/bin:$HOME/go/bin:$PATH
echo "âœ… Go ready"

# --- Step 3: Install protoc ---
echo ""
echo "ðŸ“Œ Step 3: Installing protoc..."
if ! command -v protoc &>/dev/null; then
    wget -q https://github.com/protocolbuffers/protobuf/releases/download/v25.1/protoc-25.1-linux-x86_64.zip
    sudo unzip -q -o protoc-25.1-linux-x86_64.zip -d /usr/local
    rm protoc-25.1-linux-x86_64.zip
fi
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest 2>/dev/null
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest 2>/dev/null
echo "âœ… protoc ready"

# --- Step 4: Install ONNX Runtime ---
echo ""
echo "ðŸ“Œ Step 4: Installing ONNX Runtime..."
ONNX_VERSION="1.17.0"
if [ ! -d "/usr/local/onnxruntime" ]; then
    wget -q "https://github.com/microsoft/onnxruntime/releases/download/v${ONNX_VERSION}/onnxruntime-linux-x64-gpu-${ONNX_VERSION}.tgz" \
         -O onnxruntime.tgz
    sudo mkdir -p /usr/local/onnxruntime
    sudo tar -xzf onnxruntime.tgz -C /usr/local/onnxruntime --strip-components=1
    rm onnxruntime.tgz
    echo "/usr/local/onnxruntime/lib" | sudo tee /etc/ld.so.conf.d/onnxruntime.conf >/dev/null
    sudo ldconfig 2>/dev/null
fi
export CGO_CFLAGS="-I/usr/local/onnxruntime/include"
export CGO_LDFLAGS="-L/usr/local/onnxruntime/lib -lonnxruntime"
export LD_LIBRARY_PATH="/usr/local/onnxruntime/lib:${LD_LIBRARY_PATH:-}"
echo "âœ… ONNX Runtime ready"

# --- Step 5: Download ResNet-50 ---
echo ""
echo "ðŸ“Œ Step 5: Downloading ResNet-50..."
mkdir -p models
if [ ! -f "models/resnet50.onnx" ]; then
    wget -q "https://github.com/onnx/models/raw/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx" \
         -O models/resnet50.onnx 2>/dev/null || echo "âš ï¸  Model download failed â€” using simulation"
fi
[ -f "models/resnet50.onnx" ] && echo "âœ… ResNet-50 ready ($(du -h models/resnet50.onnx | awk '{print $1}'))"

# --- Step 6: Build worker binary ---
echo ""
echo "ðŸ“Œ Step 6: Building worker binary..."
bash scripts/gen-proto.sh
go mod tidy 2>/dev/null

if CGO_ENABLED=1 go build -tags "onnx,nvml" -o bin/worker ./cmd/worker/ 2>/dev/null; then
    echo "âœ… Worker built with REAL ONNX + NVML"
    EXECUTOR_TYPE="onnx"
else
    echo "âš ï¸  CGo failed â€” building simulation"
    go build -o bin/worker ./cmd/worker/
    EXECUTOR_TYPE="simulation"
fi

# --- Step 7: Start 3 workers ---
echo ""
echo "ðŸ“Œ Step 7: Starting 3 GPU workers..."
pkill -f "bin/worker" 2>/dev/null || true
sleep 1

export ONNX_MODEL_PATH="$(pwd)/models/resnet50.onnx"

for i in 1 2 3; do
    GRPC_PORT=$((50051 + i))
    METRICS_PORT=$((9090 + i))
    
    WORKER_ID="worker-${i}" \
    WORKER_PORT="${GRPC_PORT}" \
    METRICS_PORT="${METRICS_PORT}" \
    MAX_BATCH_SIZE=32 \
    MAX_WAIT_MS=50 \
    EXECUTOR_TYPE="${EXECUTOR_TYPE}" \
    USE_NVML=true \
    LD_LIBRARY_PATH="/usr/local/onnxruntime/lib:${LD_LIBRARY_PATH:-}" \
    nohup ./bin/worker > /tmp/worker-${i}.log 2>&1 &
    
    echo "   âš¡ Worker-${i} on :${GRPC_PORT}"
done
sleep 3

# --- Step 8: Expose workers via ngrok ---
echo ""
echo "ðŸ“Œ Step 8: Exposing workers via ngrok..."
pip install -q pyngrok 2>/dev/null

python3 << 'PYTHON_SCRIPT'
import json
from pyngrok import ngrok

tunnels = {}
for i in range(1, 4):
    port = 50051 + i
    try:
        tunnel = ngrok.connect(port, "tcp")
        public_url = tunnel.public_url.replace("tcp://", "")
        tunnels[f"worker-{i}"] = public_url
        print(f"   âœ… Worker-{i} (:{ port }) â†’ {public_url}")
    except Exception as e:
        print(f"   âŒ Worker-{i} ngrok failed: {e}")

if tunnels:
    endpoints = ",".join(tunnels.values())
    print("")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("ðŸŽ‰ WORKERS ARE LIVE ON REAL GPU!")
    print("")
    print("Copy this command and run it on YOUR LOCAL machine:")
    print("")
    print(f"   WORKER_ENDPOINTS={endpoints} \\")
    print(f"   go run ./cmd/router/")
    print("")
    print("Then open: http://localhost:8080")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
else:
    print("")
    print("âŒ ngrok failed. Sign up free at https://dashboard.ngrok.com/signup")
    print("   Then run in Colab: !ngrok authtoken YOUR_TOKEN")
    print("   Then re-run: !bash scripts/colab_workers.sh")

PYTHON_SCRIPT

echo ""
echo "ðŸ“‹ Worker logs: cat /tmp/worker-1.log"
echo "ðŸ” GPU status:  nvidia-smi -l 1"
